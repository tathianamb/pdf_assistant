{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathianamb/pdf_assistant/blob/gemini_v3/chatpdf_gemini_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlQbPyPaF1I8",
        "outputId": "64157878-aba4-4821-ffbc-e302318fb2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.7/116.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m681.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet langchain-google-genai langchain faiss-cpu pypdf sentence-transformers PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juo_vUUYZSnH",
        "outputId": "c0772815-df52-4fd4-fb9c-3ce5aabd8950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m102.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet scann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnpQANReZ03x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import ScaNN\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing GOOGLE_API_KEY environment variable\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyRsV9yYFuMl"
      },
      "outputs": [],
      "source": [
        "class pdfQA:\n",
        "\n",
        "  def __init__(self, model_path=\"models/embedding-001\"):\n",
        "    self.embeddings = GoogleGenerativeAIEmbeddings(model=model_path)\n",
        "    self.chain = self._get_conversational_chain()\n",
        "\n",
        "\n",
        "  def _get_pdf_text(self, pdf_docs):\n",
        "      \"\"\"Extracts text from a list of PDF documents.\"\"\"\n",
        "      text = \"\"\n",
        "      for pdf in pdf_docs:\n",
        "          pdf_reader = PdfReader(pdf)\n",
        "          for page in pdf_reader.pages:\n",
        "              text += page.extract_text()\n",
        "      return text\n",
        "\n",
        "\n",
        "  def _get_text_chunks(self, text):\n",
        "      \"\"\"Splits text into chunks for embedding.\"\"\"\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "      chunks = text_splitter.split_text(text)\n",
        "      return chunks\n",
        "\n",
        "\n",
        "  def _create_vector_store(self, text_chunks, pdf_filename):\n",
        "      \"\"\"Creates a SCANN vector store from text chunks and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "      db = ScaNN.from_texts(text_chunks, embeddings)\n",
        "      vector_store_path = os.path.join(\"scan_vs\", pdf_filename)\n",
        "      db.save_local(vector_store_path)\n",
        "\n",
        "\n",
        "  def _get_conversational_chain(self):\n",
        "      \"\"\"Defines the question-answering chain using a prompt template and model.\"\"\"\n",
        "      prompt_template = \"\"\"\n",
        "      Answer the question with full context details.\\n\\n\n",
        "      Context:\\n {context}?\\n\n",
        "      Question: \\n{question}\\n\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "\n",
        "      model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "      prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "      chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "      return chain\n",
        "\n",
        "\n",
        "  def _answer_user_question(self, user_question, pdf_filename):\n",
        "      \"\"\"Try to answers the user's question using the conversational chain and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "      vector_store_path = os.path.join(\"scan_vs\", pdf_filename)  # Construct the full path\n",
        "\n",
        "      new_db = ScaNN.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "      docs = new_db.similarity_search(user_question)\n",
        "\n",
        "      chain = self.chain\n",
        "\n",
        "      response = chain(\n",
        "          {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
        "      )\n",
        "\n",
        "      return response[\"output_text\"]\n",
        "\n",
        "\n",
        "  def _vector_store_exists(self, pdf_filename):\n",
        "      \"\"\"Checks if a vector store exists for a given PDF filename.\"\"\"\n",
        "      vector_store_path = os.path.join(\"scan_vs\", pdf_filename)  # Construct the full path\n",
        "      return os.path.exists(vector_store_path)\n",
        "\n",
        "\n",
        "  def main(self):\n",
        "      \"\"\"Handles user interaction, PDF processing, and question answering.\"\"\"\n",
        "\n",
        "      print(\"Chat with PDF using Gemini\")\n",
        "\n",
        "      try:\n",
        "          existing_vector_stores = os.listdir(\"scan_vs\")\n",
        "          print(\"Choose a vector store:\")\n",
        "          for idx, vector_store in enumerate(existing_vector_stores):\n",
        "            print(f\"{idx + 1}. {vector_store}\")\n",
        "\n",
        "          selected = input(\"Enter the number corresponding to the vector store you want or 'upload' to a new file\\n\")\n",
        "\n",
        "      except FileNotFoundError:\n",
        "          os.makedirs(\"scan_vs\")\n",
        "          existing_vector_stores = []\n",
        "          selected = input(\"Write 'upload' to a new file\\n\")\n",
        "\n",
        "      if selected == 'upload':\n",
        "\n",
        "          pdf_docs = input(\"\\nEnter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine\\n\")\n",
        "\n",
        "          pdf_docs = pdf_docs.split(\",\")\n",
        "\n",
        "          for pdf in pdf_docs:\n",
        "              pdf_filename = os.path.splitext(os.path.basename(pdf))[0]\n",
        "              if not self._vector_store_exists(pdf_filename):\n",
        "                  raw_text = self._get_pdf_text([pdf])\n",
        "                  text_chunks = self._get_text_chunks(raw_text)\n",
        "                  pdf_filename = os.path.splitext(os.path.basename(pdf))[0]\n",
        "                  self._create_vector_store(text_chunks, pdf_filename)\n",
        "                  print(f\"\\nVector store created for {pdf_filename}.\")\n",
        "              else:\n",
        "                  print(f\"\\nVector store already exists for {pdf_filename}.\")\n",
        "\n",
        "      else:\n",
        "        selected_index = int(selected) - 1\n",
        "\n",
        "        selected_vector_store = existing_vector_stores[selected_index]\n",
        "\n",
        "        pdf_filename = selected_vector_store.replace(\"_scann_index\", \"\")\n",
        "\n",
        "        print(f\"\\nSelected vector store: {selected_vector_store}\")\n",
        "\n",
        "      print(\"Ask a Question from the PDF Files (or 'quit')\\n\")\n",
        "      while True:\n",
        "          user_question = input(\"User: \")\n",
        "          if user_question.lower() == 'quit':\n",
        "              break\n",
        "          answer = self._answer_user_question(user_question, pdf_filename)\n",
        "          # You can optionally display the answer here (if not printed earlier)\n",
        "          print(f\"\\nGemini: \\n{answer}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkIeYuncRiC",
        "outputId": "2408473d-3319-4c90-dc92-10bdbc408947"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with PDF using Gemini\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    qa_system = pdfQA()\n",
        "    qa_system.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c6lQksN4Oto"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPYNnmJ52GK4mlPpPrm5MH8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}