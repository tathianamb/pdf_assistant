{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathianamb/pdf_assistant/blob/gemini_v3/chatpdf_gemini_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlQbPyPaF1I8",
        "outputId": "a26721dc-0f20-453f-efca-842032b34f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.6/120.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet langchain-google-genai langchain faiss-cpu pypdf sentence-transformers PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juo_vUUYZSnH",
        "outputId": "d9ebbe38-dc45-4aea-f3aa-615fb6d56dcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m61.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet scann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "SnpQANReZ03x",
        "outputId": "3e41e7d5-e270-4265-a49c-0f333f54a6e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import ScaNN\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing GOOGLE_API_KEY environment variable\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oyRsV9yYFuMl"
      },
      "outputs": [],
      "source": [
        "class pdfQA:\n",
        "\n",
        "  def __init__(self, model_path=\"models/embedding-001\"):\n",
        "    self.embeddings = GoogleGenerativeAIEmbeddings(model=model_path)\n",
        "    self.chain = self._get_conversational_chain()\n",
        "\n",
        "\n",
        "  def _get_pdf_text(self, pdf_docs):\n",
        "      \"\"\"Extracts text from a list of PDF documents.\"\"\"\n",
        "      text = \"\"\n",
        "      for pdf in pdf_docs:\n",
        "          pdf_reader = PdfReader(pdf)\n",
        "          for page in pdf_reader.pages:\n",
        "              text += page.extract_text()\n",
        "      return text\n",
        "\n",
        "\n",
        "  def _get_text_chunks(self, text):\n",
        "      \"\"\"Splits text into chunks for embedding.\"\"\"\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "      chunks = text_splitter.split_text(text)\n",
        "      return chunks\n",
        "\n",
        "\n",
        "  def _create_vector_store(self, text_chunks, pdf_filename):\n",
        "      \"\"\"Creates a SCANN vector store from text chunks and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "      db = ScaNN.from_texts(text_chunks, embeddings)\n",
        "      vector_store_path = os.path.join(\"/content/gdrive/My Drive/Colab Notebooks/scann_vs\", pdf_filename)\n",
        "      db.save_local(vector_store_path)\n",
        "\n",
        "\n",
        "  def _get_conversational_chain(self):\n",
        "      \"\"\"Defines the question-answering chain using a prompt template and model.\"\"\"\n",
        "      prompt_template = \"\"\"\n",
        "      Answer the question with full context details.\\n\\n\n",
        "      Context:\\n {context}?\\n\n",
        "      Question: \\n{question}\\n\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "\n",
        "      model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.7)\n",
        "\n",
        "      prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "      chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "      return chain\n",
        "\n",
        "\n",
        "  def _answer_user_question(self, user_question, pdf_filename):\n",
        "      \"\"\"Try to answers the user's question using the conversational chain and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "      vector_store_path = os.path.join(\"/content/gdrive/My Drive/Colab Notebooks/scann_vs\", pdf_filename)\n",
        "\n",
        "      new_db = ScaNN.load_local(vector_store_path, embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "      docs = new_db.similarity_search(user_question)\n",
        "\n",
        "      chain = self.chain\n",
        "\n",
        "      response = chain(\n",
        "          {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
        "      )\n",
        "\n",
        "      return response[\"output_text\"]\n",
        "\n",
        "\n",
        "  def _vector_store_exists(self, pdf_filename):\n",
        "      \"\"\"Checks if a vector store exists for a given PDF filename.\"\"\"\n",
        "      vector_store_path = os.path.join(\"scan_vs\", pdf_filename)  # Construct the full path\n",
        "      return os.path.exists(vector_store_path)\n",
        "\n",
        "\n",
        "  def main(self):\n",
        "      \"\"\"Handles user interaction, PDF processing, and question answering.\"\"\"\n",
        "\n",
        "      print(\"Chat with PDF using Gemini\")\n",
        "\n",
        "      try:\n",
        "          existing_vector_stores = os.listdir(\"/content/gdrive/My Drive/Colab Notebooks/scann_vs\")\n",
        "          print(\"Choose a vector store:\")\n",
        "          for idx, vector_store in enumerate(existing_vector_stores):\n",
        "            print(f\"{idx + 1}. {vector_store}\")\n",
        "\n",
        "          selected = input(\"Enter the number corresponding to the vector store you want OR 'upload'\\n\")\n",
        "\n",
        "      except FileNotFoundError:\n",
        "          selected = 'upload'\n",
        "\n",
        "      if selected == 'upload':\n",
        "\n",
        "          pdf_docs = input(\"Enter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine\\n\")\n",
        "\n",
        "          pdf_docs = pdf_docs.split(\",\")\n",
        "\n",
        "          for pdf in pdf_docs:\n",
        "              pdf_filename = os.path.splitext(os.path.basename(pdf))[0]\n",
        "              if not self._vector_store_exists(pdf_filename):\n",
        "                  raw_text = self._get_pdf_text([pdf])\n",
        "                  text_chunks = self._get_text_chunks(raw_text)\n",
        "                  pdf_filename = os.path.splitext(os.path.basename(pdf))[0]\n",
        "                  self._create_vector_store(text_chunks, pdf_filename)\n",
        "                  print(f\"\\nVector store created for {pdf_filename}.\")\n",
        "              else:\n",
        "                  print(f\"\\nVector store already exists for {pdf_filename}.\")\n",
        "\n",
        "      else:\n",
        "        selected_index = int(selected) - 1\n",
        "\n",
        "        selected_vector_store = existing_vector_stores[selected_index]\n",
        "\n",
        "        pdf_filename = selected_vector_store.replace(\"_scann_index\", \"\")\n",
        "\n",
        "        print(f\"Selected vector store: {selected_vector_store}\")\n",
        "\n",
        "      print(\"Ask a Question from the PDF Files (or 'quit')\\n\")\n",
        "      while True:\n",
        "          user_question = input(\"User: \")\n",
        "          if user_question.lower() == 'quit':\n",
        "              break\n",
        "          answer = self._answer_user_question(user_question, pdf_filename)\n",
        "          # You can optionally display the answer here (if not printed earlier)\n",
        "          print(f\"\\nGemini: \\n{answer}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkIeYuncRiC",
        "outputId": "bdf0993f-a201-4f71-f5f5-9301aeee37c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with PDF using Gemini\n",
            "Choose a vector store:\n",
            "1. An_Extensive_Comparison_of_Linear_Models__Machine_Learning_approaches_and_combination_methods_for_Wind_Speed_Forecasting\n",
            "Enter the number corresponding to the vector store you want OR 'upload'\n",
            "upload\n",
            "Enter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine\n",
            "/content/article.pdf\n",
            "\n",
            "Vector store created for article.\n",
            "Ask a Question from the PDF Files (or 'quit')\n",
            "\n",
            "User: test\n",
            "\n",
            "Gemini: \n",
            "The Wilcoxon pairwised test was applied to determine if the winner presents a sta- 361\n",
            "tistical difference from the other models. In all cases, the p-value achieved is very small, 362\n",
            "much below 0.05. Figure 3 shows the boxplot graphic for S5 considering the MSE of 30 363\n",
            "simulations of each model that uses neural approaches (single or ensembles) for the test 364\n",
            "set. This figure allows analyzing the dispersion of the results in which the ELM and ESN 365\n",
            "architectures achieved lower average MSE and exhibited greater consistency in perfor- 366\n",
            "mance across simulations. On the other hand, the RBF network exhibited greater dispersion. 367\n",
            "Furthermore, the integration of AR and ARMA contributed positively only for RBF. 368\n",
            "\n",
            "\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    qa_system = pdfQA()\n",
        "    qa_system.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8c6lQksN4Oto"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFQxmEN14il11HzUocNS8D",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}