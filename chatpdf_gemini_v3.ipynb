{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tathianamb/pdf_assistant/blob/gemini_v3/chatpdf_gemini_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlQbPyPaF1I8",
        "outputId": "7d5d14ef-b450-4ad2-c926-f9725f890558"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.6/867.6 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.5/142.5 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet langchain-google-genai langchain faiss-cpu pypdf sentence-transformers PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juo_vUUYZSnH",
        "outputId": "5b6f7ed1-1caf-4f70-e1a6-063472d6c053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.6/10.6 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m589.8/589.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.2/311.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tf-keras 2.15.1 requires tensorflow<2.16,>=2.15, but you have tensorflow 2.16.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet scann"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SnpQANReZ03x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import ScaNN\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing GOOGLE_API_KEY environment variable\")\n",
        "\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oyRsV9yYFuMl"
      },
      "outputs": [],
      "source": [
        "class pdfQA:\n",
        "\n",
        "  def __init__(self, model_path=\"models/embedding-001\"):\n",
        "    self.embeddings = GoogleGenerativeAIEmbeddings(model=model_path)\n",
        "    self.chain = self._get_conversational_chain()\n",
        "\n",
        "  def _get_pdf_text(self, pdf_docs):\n",
        "      \"\"\"Extracts text from a list of PDF documents.\"\"\"\n",
        "      text = \"\"\n",
        "      for pdf in pdf_docs:\n",
        "          pdf_reader = PdfReader(pdf)\n",
        "          for page in pdf_reader.pages:\n",
        "              text += page.extract_text()\n",
        "      return text\n",
        "\n",
        "\n",
        "  def _get_text_chunks(self, text):\n",
        "      \"\"\"Splits text into chunks for embedding.\"\"\"\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "      chunks = text_splitter.split_text(text)\n",
        "      return chunks\n",
        "\n",
        "\n",
        "  def _create_vector_store(self, text_chunks):\n",
        "      \"\"\"Creates a SCANN vector store from text chunks and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "      db = ScaNN.from_texts(text_chunks, embeddings)\n",
        "      db.save_local(\"scann_index\")\n",
        "\n",
        "\n",
        "  def _get_conversational_chain(self):\n",
        "      \"\"\"Defines the question-answering chain using a prompt template and model.\"\"\"\n",
        "      prompt_template = \"\"\"\n",
        "      Answer the question with full context details. If you don't know the answer say \"The context doesn't tell you about\".\\n\\n\n",
        "      Context:\\n {context}?\\n\n",
        "      Question: \\n{question}\\n\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "\n",
        "      model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "      prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "      chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "      return chain\n",
        "\n",
        "\n",
        "  def _answer_user_question(self, user_question):\n",
        "      \"\"\"Answers the user's question using the conversational chain and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "      new_db = ScaNN.load_local(\"scann_index\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "      docs = new_db.similarity_search(user_question)\n",
        "\n",
        "      chain = self.chain\n",
        "\n",
        "      response = chain(\n",
        "          {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
        "      )\n",
        "\n",
        "      return response[\"output_text\"]\n",
        "\n",
        "\n",
        "  def main(self):\n",
        "      \"\"\"Handles user interaction, PDF processing, and question answering.\"\"\"\n",
        "      # Consider using Colab file uploader (e.g., from google.colab import files)\n",
        "\n",
        "      print(\"Chat with PDF using Gemini\")\n",
        "\n",
        "      # Load vector store or create it if it doesn't exist\n",
        "      if not os.path.exists(\"scann_index\"):\n",
        "          print(\"Vector store not found. Processing PDFs to create it...\")\n",
        "          pdf_docs = input(\"Enter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine: \")\n",
        "          if pdf_docs == \"upload\":\n",
        "              # Implement upload functionality using Colab's file uploader\n",
        "              pass  # Replace with upload logic\n",
        "          else:\n",
        "              pdf_docs = pdf_docs.split(\",\")\n",
        "\n",
        "          raw_text = self._get_pdf_text(pdf_docs)\n",
        "          text_chunks = self._get_text_chunks(raw_text)\n",
        "          self._create_vector_store(text_chunks)\n",
        "          print(\"Vector store created.\")\n",
        "\n",
        "      print(\"Ask a Question from the PDF Files (or 'quit')\\n\")\n",
        "      while True:\n",
        "          user_question = input(\"User: \")\n",
        "          if user_question.lower() == 'quit':\n",
        "              break\n",
        "          answer = self._answer_user_question(user_question)\n",
        "          # You can optionally display the answer here (if not printed earlier)\n",
        "          print(f\"\\nGemini: \\n{answer}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDkIeYuncRiC",
        "outputId": "0dfa08ed-491a-4dde-ed0a-d9820e58dc99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with PDF using Gemini\n",
            "Vector store not found. Processing PDFs to create it...\n",
            "Enter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine: /content/An_Extensive_Comparison_of_Linear_Models__Machine_Learning_approaches_and_combination_methods_for_Wind_Speed_Forecasting.pdf\n",
            "Vector store created.\n",
            "Ask a Question from the PDF Files (or 'quit')\n",
            "\n",
            "User: Explain to me what a global model is and what a local model is\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Gemini: \n",
            "The context does not mention anything about global or local models, so I cannot answer this question from the provided context.\n",
            "\n",
            "\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    qa_system = pdfQA()\n",
        "    qa_system.main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "akiiLs6hcvbB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqMxuk3UxuN+ZOPVynZ0v0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}