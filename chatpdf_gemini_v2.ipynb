{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlQbPyPaF1I8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "804bd929-f1de-4e7e-e2c8-048e31147e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.7/817.7 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m51.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.4/116.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m686.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "%pip install -U --quiet langchain-google-genai langchain faiss-cpu pypdf sentence-transformers PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnbSvVJCZgl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oyRsV9yYFuMl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50qLq0GRI3ZJ"
      },
      "outputs": [],
      "source": [
        "api_key = userdata.get('GOOGLE_API_KEY')\n",
        "if not api_key:\n",
        "    raise ValueError(\"Missing GOOGLE_API_KEY environment variable\")\n",
        "\n",
        "genai.configure(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izl-gOODKXOB"
      },
      "outputs": [],
      "source": [
        "os.environ[\"GOOGLE_API_KEY\"] = api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43gHZkGMFLwJ"
      },
      "outputs": [],
      "source": [
        "class pdfQA:\n",
        "\n",
        "  def __init__(self, model_path=\"models/embedding-001\"):\n",
        "    self.embeddings = GoogleGenerativeAIEmbeddings(model=model_path)\n",
        "    self.chain = self._get_conversational_chain()\n",
        "\n",
        "\n",
        "  def _get_pdf_text(self, pdf_docs):\n",
        "      \"\"\"Extracts text from a list of PDF documents.\"\"\"\n",
        "      text = \"\"\n",
        "      for pdf in pdf_docs:\n",
        "          pdf_reader = PdfReader(pdf)\n",
        "          for page in pdf_reader.pages:\n",
        "              text += page.extract_text()\n",
        "      return text\n",
        "\n",
        "\n",
        "  def _get_text_chunks(self, text):\n",
        "      \"\"\"Splits text into chunks for embedding.\"\"\"\n",
        "      text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)\n",
        "      chunks = text_splitter.split_text(text)\n",
        "      return chunks\n",
        "\n",
        "\n",
        "  def _create_vector_store(self, text_chunks):\n",
        "      \"\"\"Creates a FAISS vector store from text chunks and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "      vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "      vector_store.save_local(\"faiss_index\")\n",
        "\n",
        "\n",
        "  def _get_conversational_chain(self):\n",
        "      \"\"\"Defines the question-answering chain using a prompt template and model.\"\"\"\n",
        "      prompt_template = \"\"\"\n",
        "      Answer the question with full context details. If you don't know the answer say \"The context doesn't tell you about\".\\n\\n\n",
        "      Context:\\n {context}?\\n\n",
        "      Question: \\n{question}\\n\n",
        "\n",
        "      Answer:\n",
        "      \"\"\"\n",
        "\n",
        "      model = ChatGoogleGenerativeAI(model=\"gemini-pro\", temperature=0.3)\n",
        "\n",
        "      prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "      chain = load_qa_chain(model, chain_type=\"stuff\", prompt=prompt)\n",
        "\n",
        "      return chain\n",
        "\n",
        "\n",
        "  def _answer_user_question(self, user_question):\n",
        "      \"\"\"Answers the user's question using the conversational chain and embeddings.\"\"\"\n",
        "      embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "      new_db = FAISS.load_local(\"faiss_index\", embeddings=embeddings, allow_dangerous_deserialization=True)\n",
        "      docs = new_db.similarity_search(user_question)\n",
        "\n",
        "      chain = self.chain\n",
        "\n",
        "      response = chain(\n",
        "          {\"input_documents\": docs, \"question\": user_question}, return_only_outputs=True\n",
        "      )\n",
        "\n",
        "      return response[\"output_text\"]\n",
        "\n",
        "\n",
        "  def main(self):\n",
        "      \"\"\"Handles user interaction, PDF processing, and question answering.\"\"\"\n",
        "      # Consider using Colab file uploader (e.g., from google.colab import files)\n",
        "\n",
        "      print(\"Chat with PDF using Gemini\")\n",
        "\n",
        "      # Load vector store or create it if it doesn't exist\n",
        "      if not os.path.exists(\"faiss_index\"):\n",
        "          print(\"Vector store not found. Processing PDFs to create it...\")\n",
        "          pdf_docs = input(\"Enter paths to your PDF files separated by commas (,) or 'upload' to upload from local machine: \")\n",
        "          if pdf_docs == \"upload\":\n",
        "              # Implement upload functionality using Colab's file uploader\n",
        "              pass  # Replace with upload logic\n",
        "          else:\n",
        "              pdf_docs = pdf_docs.split(\",\")\n",
        "\n",
        "          raw_text = self._get_pdf_text(pdf_docs)\n",
        "          text_chunks = self._get_text_chunks(raw_text)\n",
        "          self._create_vector_store(text_chunks)\n",
        "          print(\"Vector store created.\")\n",
        "\n",
        "      print(\"Ask a Question from the PDF Files (or 'quit')\\n\")\n",
        "      while True:\n",
        "          user_question = input(\"User: \")\n",
        "          if user_question.lower() == 'quit':\n",
        "              break\n",
        "          answer = self._answer_user_question(user_question)\n",
        "          # You can optionally display the answer here (if not printed earlier)\n",
        "          print(f\"\\nGemini: \\n{answer}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fyk51gjUI6hv",
        "outputId": "fcf556f2-7f9a-4ba4-9f08-724ba6542ec0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat with PDF using Gemini\n",
            "Ask a Question from the PDF Files (or 'quit')\n",
            "\n",
            "User: summarize the case study and results\n",
            "\n",
            "Gemini: \n",
            "The case study evaluated the performance of different forecasting models for one-step-ahead forecasting of wind speed. Five time series from different cities in Brazil were used, and the models included linear models (AR and ARMA), neural networks (MLP, RBF, ELM, and ESN), hybrid models (AR+ANN, ARMA+ANN), and ensemble models. The results showed that the ensemble models generally performed better than the single models, with the Median Ensemble of all single models except the RBF being the best overall predictor. The ARMA model performed well for series with lower coefficient of variation, while the Ensemble Median SM-RBF performed better for series with higher coefficient of variation and magnitudes. The study highlights the importance of considering different forecasting models and ensemble approaches to improve the accuracy of wind speed forecasting for renewable energy applications.\n",
            "\n",
            "\n",
            "User: quit\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  qa_system = pdfQA()\n",
        "  qa_system.main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VBUjP2Az6FgC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}